# ç›‘æ§ä¸è°ƒè¯•é¢è¯•é¢˜

[â† è¿”å›åç«¯é¢è¯•é¢˜ç›®å½•](./README.md)

## ğŸ“‹ ç›®å½•

- [æ—¥å¿—ç³»ç»Ÿ](#æ—¥å¿—ç³»ç»Ÿ)
- [æ€§èƒ½æŒ‡æ ‡](#æ€§èƒ½æŒ‡æ ‡)
- [é“¾è·¯è¿½è¸ª](#é“¾è·¯è¿½è¸ª)
- [é”™è¯¯ç›‘æ§](#é”™è¯¯ç›‘æ§)
- [è°ƒè¯•æŠ€å·§](#è°ƒè¯•æŠ€å·§)
- [å®æˆ˜æ¡ˆä¾‹](#å®æˆ˜æ¡ˆä¾‹)

## ğŸ¯ æ ¸å¿ƒçŸ¥è¯†ç‚¹

```mermaid
mindmap
  root((ç›‘æ§ä¸è°ƒè¯•))
    æ—¥å¿—ç³»ç»Ÿ
      ç»“æ„åŒ–æ—¥å¿—
      æ—¥å¿—çº§åˆ«
      æ—¥å¿—èšåˆ
    æ€§èƒ½æŒ‡æ ‡
      ç³»ç»ŸæŒ‡æ ‡
      ä¸šåŠ¡æŒ‡æ ‡
      è‡ªå®šä¹‰æŒ‡æ ‡
    é“¾è·¯è¿½è¸ª
      åˆ†å¸ƒå¼è¿½è¸ª
      è°ƒç”¨é“¾åˆ†æ
      æ€§èƒ½ç“¶é¢ˆ
    é”™è¯¯ç›‘æ§
      å¼‚å¸¸æ•è·
      é”™è¯¯æŠ¥è­¦
      æ•…éšœæ¢å¤
```

## æ—¥å¿—ç³»ç»Ÿ

### ğŸ’¡ åˆçº§é¢˜ç›®

#### 1. æ—¥å¿—çº§åˆ«çš„åˆ†ç±»å’Œä½¿ç”¨åœºæ™¯ï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹ï¼š**
- **TRACE**ï¼šæœ€è¯¦ç»†çš„ä¿¡æ¯ï¼Œé€šå¸¸åªåœ¨å¼€å‘æ—¶ä½¿ç”¨
- **DEBUG**ï¼šè°ƒè¯•ä¿¡æ¯ï¼Œç”¨äºé—®é¢˜è¯Šæ–­
- **INFO**ï¼šä¸€èˆ¬ä¿¡æ¯ï¼Œè®°å½•ç¨‹åºè¿è¡ŒçŠ¶æ€
- **WARN**ï¼šè­¦å‘Šä¿¡æ¯ï¼Œå¯èƒ½çš„é—®é¢˜
- **ERROR**ï¼šé”™è¯¯ä¿¡æ¯ï¼Œéœ€è¦å…³æ³¨ä½†ä¸å½±å“ç¨‹åºè¿è¡Œ
- **FATAL**ï¼šè‡´å‘½é”™è¯¯ï¼Œç¨‹åºæ— æ³•ç»§ç»­è¿è¡Œ

```python
import logging
import json
from datetime import datetime
from typing import Dict, Any

# é…ç½®ç»“æ„åŒ–æ—¥å¿—
class StructuredLogger:
    def __init__(self, name: str, level: str = "INFO"):
        self.logger = logging.getLogger(name)
        self.logger.setLevel(getattr(logging, level.upper()))
        
        # åˆ›å»ºæ ¼å¼åŒ–å™¨
        formatter = logging.Formatter(
            '%(asctime)s - %(name)s - %(levelname)s - %(message)s'
        )
        
        # æ§åˆ¶å°å¤„ç†å™¨
        console_handler = logging.StreamHandler()
        console_handler.setFormatter(formatter)
        self.logger.addHandler(console_handler)
        
        # æ–‡ä»¶å¤„ç†å™¨
        file_handler = logging.FileHandler('app.log')
        file_handler.setFormatter(formatter)
        self.logger.addHandler(file_handler)
    
    def _log_structured(self, level: str, message: str, **kwargs):
        """è®°å½•ç»“æ„åŒ–æ—¥å¿—"""
        log_data = {
            'timestamp': datetime.utcnow().isoformat(),
            'level': level,
            'message': message,
            'service': 'backend-service',
            **kwargs
        }
        
        getattr(self.logger, level.lower())(json.dumps(log_data))
    
    def info(self, message: str, **kwargs):
        self._log_structured('INFO', message, **kwargs)
    
    def error(self, message: str, **kwargs):
        self._log_structured('ERROR', message, **kwargs)
    
    def warning(self, message: str, **kwargs):
        self._log_structured('WARNING', message, **kwargs)
    
    def debug(self, message: str, **kwargs):
        self._log_structured('DEBUG', message, **kwargs)

# ä½¿ç”¨ç¤ºä¾‹
logger = StructuredLogger('user_service')

def process_user_request(user_id: int, action: str):
    logger.info(
        "Processing user request",
        user_id=user_id,
        action=action,
        request_id="req-123"
    )
    
    try:
        # æ¨¡æ‹Ÿä¸šåŠ¡é€»è¾‘
        if action == "delete" and user_id == 1:
            raise ValueError("Cannot delete admin user")
        
        logger.info(
            "User request completed successfully",
            user_id=user_id,
            action=action,
            duration_ms=150
        )
        
    except Exception as e:
        logger.error(
            "User request failed",
            user_id=user_id,
            action=action,
            error=str(e),
            error_type=type(e).__name__
        )
        raise
```

#### 2. å¦‚ä½•è®¾è®¡é«˜æ€§èƒ½çš„æ—¥å¿—ç³»ç»Ÿï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹ï¼š**
- **å¼‚æ­¥å†™å…¥**ï¼šé¿å…é˜»å¡ä¸»çº¿ç¨‹
- **æ‰¹é‡å†™å…¥**ï¼šå‡å°‘ I/O æ“ä½œ
- **æ—¥å¿—è½®è½¬**ï¼šæ§åˆ¶æ—¥å¿—æ–‡ä»¶å¤§å°
- **ç¼“å†²æœºåˆ¶**ï¼šæé«˜å†™å…¥æ•ˆç‡

```python
import asyncio
import aiofiles
from collections import deque
from threading import Thread, Lock
import time
import os

class AsyncLogger:
    def __init__(self, filename: str, max_size: int = 10*1024*1024, 
                 backup_count: int = 5, batch_size: int = 100):
        self.filename = filename
        self.max_size = max_size
        self.backup_count = backup_count
        self.batch_size = batch_size
        
        self.log_queue = deque()
        self.queue_lock = Lock()
        self.running = True
        
        # å¯åŠ¨åå°å†™å…¥çº¿ç¨‹
        self.writer_thread = Thread(target=self._writer_loop, daemon=True)
        self.writer_thread.start()
    
    def log(self, level: str, message: str, **kwargs):
        """æ·»åŠ æ—¥å¿—åˆ°é˜Ÿåˆ—"""
        log_entry = {
            'timestamp': time.time(),
            'level': level,
            'message': message,
            **kwargs
        }
        
        with self.queue_lock:
            self.log_queue.append(json.dumps(log_entry) + '\n')
    
    def _writer_loop(self):
        """åå°å†™å…¥å¾ªç¯"""
        while self.running:
            batch = []
            
            # æ”¶é›†æ‰¹é‡æ—¥å¿—
            with self.queue_lock:
                while len(batch) < self.batch_size and self.log_queue:
                    batch.append(self.log_queue.popleft())
            
            if batch:
                self._write_batch(batch)
            else:
                time.sleep(0.1)  # æ²¡æœ‰æ—¥å¿—æ—¶çŸ­æš‚ä¼‘çœ 
    
    def _write_batch(self, batch: list):
        """æ‰¹é‡å†™å…¥æ—¥å¿—"""
        try:
            # æ£€æŸ¥æ–‡ä»¶å¤§å°ï¼Œå¿…è¦æ—¶è½®è½¬
            if os.path.exists(self.filename) and \
               os.path.getsize(self.filename) > self.max_size:
                self._rotate_logs()
            
            with open(self.filename, 'a', encoding='utf-8') as f:
                f.writelines(batch)
                f.flush()
                
        except Exception as e:
            print(f"æ—¥å¿—å†™å…¥å¤±è´¥: {e}")
    
    def _rotate_logs(self):
        """æ—¥å¿—è½®è½¬"""
        try:
            # åˆ é™¤æœ€è€çš„å¤‡ä»½
            oldest_backup = f"{self.filename}.{self.backup_count}"
            if os.path.exists(oldest_backup):
                os.remove(oldest_backup)
            
            # é‡å‘½åç°æœ‰å¤‡ä»½
            for i in range(self.backup_count - 1, 0, -1):
                old_name = f"{self.filename}.{i}"
                new_name = f"{self.filename}.{i + 1}"
                if os.path.exists(old_name):
                    os.rename(old_name, new_name)
            
            # é‡å‘½åå½“å‰æ—¥å¿—æ–‡ä»¶
            if os.path.exists(self.filename):
                os.rename(self.filename, f"{self.filename}.1")
                
        except Exception as e:
            print(f"æ—¥å¿—è½®è½¬å¤±è´¥: {e}")
    
    def close(self):
        """å…³é—­æ—¥å¿—å™¨"""
        self.running = False
        self.writer_thread.join()
        
        # å†™å…¥å‰©ä½™æ—¥å¿—
        with self.queue_lock:
            if self.log_queue:
                self._write_batch(list(self.log_queue))
                self.log_queue.clear()

# æ—¥å¿—èšåˆå™¨
class LogAggregator:
    def __init__(self):
        self.loggers = {}
        self.filters = []
    
    def add_logger(self, name: str, logger: AsyncLogger):
        self.loggers[name] = logger
    
    def add_filter(self, filter_func):
        """æ·»åŠ æ—¥å¿—è¿‡æ»¤å™¨"""
        self.filters.append(filter_func)
    
    def log(self, level: str, message: str, **kwargs):
        """å‘æ‰€æœ‰æ—¥å¿—å™¨å‘é€æ—¥å¿—"""
        # åº”ç”¨è¿‡æ»¤å™¨
        for filter_func in self.filters:
            if not filter_func(level, message, **kwargs):
                return
        
        for logger in self.loggers.values():
            logger.log(level, message, **kwargs)

# ä½¿ç”¨ç¤ºä¾‹
def setup_logging():
    # åˆ›å»ºä¸åŒç±»å‹çš„æ—¥å¿—å™¨
    app_logger = AsyncLogger('app.log')
    error_logger = AsyncLogger('error.log')
    access_logger = AsyncLogger('access.log')
    
    # åˆ›å»ºèšåˆå™¨
    aggregator = LogAggregator()
    aggregator.add_logger('app', app_logger)
    aggregator.add_logger('error', error_logger)
    aggregator.add_logger('access', access_logger)
    
    # æ·»åŠ è¿‡æ»¤å™¨
    def error_filter(level, message, **kwargs):
        return level in ['ERROR', 'FATAL']
    
    def access_filter(level, message, **kwargs):
        return 'request_id' in kwargs
    
    return aggregator
```

### ğŸ”¥ ä¸­çº§é¢˜ç›®

#### 3. å¦‚ä½•å®ç°åˆ†å¸ƒå¼æ—¥å¿—æ”¶é›†ç³»ç»Ÿï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹ï¼š**
- **æ—¥å¿—æ”¶é›†å™¨**ï¼šFluentdã€Logstashã€Filebeat
- **æ¶ˆæ¯é˜Ÿåˆ—**ï¼šKafkaã€RabbitMQ ä½œä¸ºç¼“å†²
- **å­˜å‚¨ç³»ç»Ÿ**ï¼šElasticsearchã€ClickHouse
- **å¯è§†åŒ–**ï¼šKibanaã€Grafana

```mermaid
flowchart TD
    A[åº”ç”¨æœåŠ¡å™¨ 1] --> D[æ—¥å¿—æ”¶é›†å™¨]
    B[åº”ç”¨æœåŠ¡å™¨ 2] --> D
    C[åº”ç”¨æœåŠ¡å™¨ 3] --> D
    
    D --> E[æ¶ˆæ¯é˜Ÿåˆ— Kafka]
    E --> F[æ—¥å¿—å¤„ç†å™¨]
    F --> G[Elasticsearch]
    G --> H[Kibana å¯è§†åŒ–]
    
    subgraph "æ—¥å¿—å¤„ç†æµç¨‹"
        I[åŸå§‹æ—¥å¿—] --> J[è§£æ]
        J --> K[è¿‡æ»¤]
        K --> L[è½¬æ¢]
        L --> M[å­˜å‚¨]
    end
```

```python
import json
import asyncio
from kafka import KafkaProducer, KafkaConsumer
from elasticsearch import Elasticsearch
import re
from datetime import datetime

class DistributedLogCollector:
    def __init__(self, kafka_servers: list, topic: str):
        self.producer = KafkaProducer(
            bootstrap_servers=kafka_servers,
            value_serializer=lambda v: json.dumps(v).encode('utf-8'),
            batch_size=16384,  # æ‰¹é‡å‘é€
            linger_ms=10,      # å»¶è¿Ÿå‘é€ä»¥æé«˜æ‰¹é‡æ•ˆç‡
            compression_type='gzip'
        )
        self.topic = topic
    
    def send_log(self, log_data: dict):
        """å‘é€æ—¥å¿—åˆ° Kafka"""
        try:
            self.producer.send(self.topic, log_data)
        except Exception as e:
            print(f"å‘é€æ—¥å¿—å¤±è´¥: {e}")
    
    def close(self):
        self.producer.close()

class LogProcessor:
    def __init__(self, kafka_servers: list, topic: str, es_hosts: list):
        self.consumer = KafkaConsumer(
            topic,
            bootstrap_servers=kafka_servers,
            value_deserializer=lambda m: json.loads(m.decode('utf-8')),
            group_id='log-processor-group',
            auto_offset_reset='latest'
        )
        
        self.es = Elasticsearch(es_hosts)
        self.parsers = []
    
    def add_parser(self, parser_func):
        """æ·»åŠ æ—¥å¿—è§£æå™¨"""
        self.parsers.append(parser_func)
    
    def process_logs(self):
        """å¤„ç†æ—¥å¿—æ¶ˆæ¯"""
        for message in self.consumer:
            try:
                log_data = message.value
                
                # åº”ç”¨è§£æå™¨
                for parser in self.parsers:
                    log_data = parser(log_data)
                
                # å­˜å‚¨åˆ° Elasticsearch
                self._store_log(log_data)
                
            except Exception as e:
                print(f"å¤„ç†æ—¥å¿—å¤±è´¥: {e}")
    
    def _store_log(self, log_data: dict):
        """å­˜å‚¨æ—¥å¿—åˆ° Elasticsearch"""
        index_name = f"logs-{datetime.now().strftime('%Y-%m-%d')}"
        
        try:
            self.es.index(
                index=index_name,
                body=log_data,
                doc_type='_doc'
            )
        except Exception as e:
            print(f"å­˜å‚¨æ—¥å¿—å¤±è´¥: {e}")

# æ—¥å¿—è§£æå™¨ç¤ºä¾‹
def nginx_access_parser(log_data: dict) -> dict:
    """è§£æ Nginx è®¿é—®æ—¥å¿—"""
    if 'message' not in log_data:
        return log_data
    
    # Nginx è®¿é—®æ—¥å¿—æ ¼å¼
    pattern = r'(\S+) - - \[(.*?)\] "(\S+) (\S+) (\S+)" (\d+) (\d+) "(.*?)" "(.*?)"'
    match = re.match(pattern, log_data['message'])
    
    if match:
        log_data.update({
            'client_ip': match.group(1),
            'timestamp': match.group(2),
            'method': match.group(3),
            'url': match.group(4),
            'protocol': match.group(5),
            'status_code': int(match.group(6)),
            'response_size': int(match.group(7)),
            'referer': match.group(8),
            'user_agent': match.group(9),
            'log_type': 'nginx_access'
        })
    
    return log_data

def application_error_parser(log_data: dict) -> dict:
    """è§£æåº”ç”¨é”™è¯¯æ—¥å¿—"""
    if log_data.get('level') == 'ERROR':
        # æå–é”™è¯¯ä¿¡æ¯
        message = log_data.get('message', '')
        
        # æ£€æµ‹å¸¸è§é”™è¯¯ç±»å‹
        if 'database' in message.lower():
            log_data['error_category'] = 'database'
        elif 'timeout' in message.lower():
            log_data['error_category'] = 'timeout'
        elif 'permission' in message.lower():
            log_data['error_category'] = 'permission'
        else:
            log_data['error_category'] = 'unknown'
        
        # æ·»åŠ å‘Šè­¦æ ‡è®°
        log_data['alert_required'] = True
    
    return log_data

# ä½¿ç”¨ç¤ºä¾‹
def setup_distributed_logging():
    kafka_servers = ['localhost:9092']
    es_hosts = ['localhost:9200']
    
    # åˆ›å»ºæ—¥å¿—æ”¶é›†å™¨
    collector = DistributedLogCollector(kafka_servers, 'application-logs')
    
    # åˆ›å»ºæ—¥å¿—å¤„ç†å™¨
    processor = LogProcessor(kafka_servers, 'application-logs', es_hosts)
    processor.add_parser(nginx_access_parser)
    processor.add_parser(application_error_parser)
    
    # å¯åŠ¨å¤„ç†å™¨
    import threading
    processor_thread = threading.Thread(target=processor.process_logs, daemon=True)
    processor_thread.start()
    
    return collector
```

## æ€§èƒ½æŒ‡æ ‡

### ğŸ”¥ ä¸­çº§é¢˜ç›®

#### 4. å¦‚ä½•è®¾è®¡åº”ç”¨æ€§èƒ½ç›‘æ§ç³»ç»Ÿï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹ï¼š**
- **ç³»ç»ŸæŒ‡æ ‡**ï¼šCPUã€å†…å­˜ã€ç£ç›˜ã€ç½‘ç»œ
- **åº”ç”¨æŒ‡æ ‡**ï¼šå“åº”æ—¶é—´ã€ååé‡ã€é”™è¯¯ç‡
- **ä¸šåŠ¡æŒ‡æ ‡**ï¼šç”¨æˆ·æ´»è·ƒåº¦ã€è½¬åŒ–ç‡
- **è‡ªå®šä¹‰æŒ‡æ ‡**ï¼šç‰¹å®šä¸šåŠ¡é€»è¾‘çš„æŒ‡æ ‡

```python
import time
import psutil
import threading
from collections import defaultdict, deque
from dataclasses import dataclass
from typing import Dict, List, Callable
import json

@dataclass
class Metric:
    name: str
    value: float
    timestamp: float
    tags: Dict[str, str] = None
    
    def to_dict(self):
        return {
            'name': self.name,
            'value': self.value,
            'timestamp': self.timestamp,
            'tags': self.tags or {}
        }

class MetricsCollector:
    def __init__(self, collection_interval: int = 60):
        self.collection_interval = collection_interval
        self.metrics = defaultdict(deque)
        self.custom_collectors = []
        self.running = False
        self.thread = None
    
    def start(self):
        """å¯åŠ¨æŒ‡æ ‡æ”¶é›†"""
        self.running = True
        self.thread = threading.Thread(target=self._collection_loop, daemon=True)
        self.thread.start()
    
    def stop(self):
        """åœæ­¢æŒ‡æ ‡æ”¶é›†"""
        self.running = False
        if self.thread:
            self.thread.join()
    
    def add_custom_collector(self, collector_func: Callable):
        """æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†å™¨"""
        self.custom_collectors.append(collector_func)
    
    def record_metric(self, name: str, value: float, tags: Dict[str, str] = None):
        """è®°å½•è‡ªå®šä¹‰æŒ‡æ ‡"""
        metric = Metric(name, value, time.time(), tags)
        self.metrics[name].append(metric)
        
        # ä¿æŒæœ€è¿‘1000ä¸ªæ•°æ®ç‚¹
        if len(self.metrics[name]) > 1000:
            self.metrics[name].popleft()
    
    def _collection_loop(self):
        """æŒ‡æ ‡æ”¶é›†å¾ªç¯"""
        while self.running:
            try:
                # æ”¶é›†ç³»ç»ŸæŒ‡æ ‡
                self._collect_system_metrics()
                
                # æ”¶é›†è‡ªå®šä¹‰æŒ‡æ ‡
                for collector in self.custom_collectors:
                    try:
                        collector(self)
                    except Exception as e:
                        print(f"è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
                
                time.sleep(self.collection_interval)
                
            except Exception as e:
                print(f"æŒ‡æ ‡æ”¶é›†å¤±è´¥: {e}")
    
    def _collect_system_metrics(self):
        """æ”¶é›†ç³»ç»ŸæŒ‡æ ‡"""
        timestamp = time.time()
        
        # CPU ä½¿ç”¨ç‡
        cpu_percent = psutil.cpu_percent(interval=1)
        self.record_metric('system.cpu.usage', cpu_percent, {'unit': 'percent'})
        
        # å†…å­˜ä½¿ç”¨æƒ…å†µ
        memory = psutil.virtual_memory()
        self.record_metric('system.memory.usage', memory.percent, {'unit': 'percent'})
        self.record_metric('system.memory.available', memory.available, {'unit': 'bytes'})
        
        # ç£ç›˜ä½¿ç”¨æƒ…å†µ
        disk = psutil.disk_usage('/')
        self.record_metric('system.disk.usage', disk.percent, {'unit': 'percent'})
        self.record_metric('system.disk.free', disk.free, {'unit': 'bytes'})
        
        # ç½‘ç»œ I/O
        network = psutil.net_io_counters()
        self.record_metric('system.network.bytes_sent', network.bytes_sent, {'unit': 'bytes'})
        self.record_metric('system.network.bytes_recv', network.bytes_recv, {'unit': 'bytes'})
    
    def get_metrics(self, name: str = None, limit: int = 100) -> List[Dict]:
        """è·å–æŒ‡æ ‡æ•°æ®"""
        if name:
            metrics = list(self.metrics.get(name, []))[-limit:]
            return [m.to_dict() for m in metrics]
        else:
            result = {}
            for metric_name, metric_list in self.metrics.items():
                result[metric_name] = [m.to_dict() for m in list(metric_list)[-limit:]]
            return result

# åº”ç”¨æ€§èƒ½ç›‘æ§è£…é¥°å™¨
class PerformanceMonitor:
    def __init__(self, metrics_collector: MetricsCollector):
        self.metrics_collector = metrics_collector
        self.request_times = deque(maxlen=1000)
        self.error_count = 0
        self.request_count = 0
    
    def monitor_request(self, endpoint: str = None):
        """ç›‘æ§è¯·æ±‚æ€§èƒ½çš„è£…é¥°å™¨"""
        def decorator(func):
            def wrapper(*args, **kwargs):
                start_time = time.time()
                
                try:
                    result = func(*args, **kwargs)
                    self.request_count += 1
                    
                    # è®°å½•æˆåŠŸè¯·æ±‚
                    duration = (time.time() - start_time) * 1000  # è½¬æ¢ä¸ºæ¯«ç§’
                    self.request_times.append(duration)
                    
                    tags = {'endpoint': endpoint or func.__name__, 'status': 'success'}
                    self.metrics_collector.record_metric('app.request.duration', duration, tags)
                    self.metrics_collector.record_metric('app.request.count', 1, tags)
                    
                    return result
                    
                except Exception as e:
                    self.error_count += 1
                    
                    # è®°å½•é”™è¯¯è¯·æ±‚
                    duration = (time.time() - start_time) * 1000
                    tags = {
                        'endpoint': endpoint or func.__name__, 
                        'status': 'error',
                        'error_type': type(e).__name__
                    }
                    self.metrics_collector.record_metric('app.request.duration', duration, tags)
                    self.metrics_collector.record_metric('app.request.error', 1, tags)
                    
                    raise
            
            return wrapper
        return decorator
    
    def get_performance_summary(self) -> Dict:
        """è·å–æ€§èƒ½æ‘˜è¦"""
        if not self.request_times:
            return {}
        
        times = list(self.request_times)
        times.sort()
        
        return {
            'total_requests': self.request_count,
            'error_count': self.error_count,
            'error_rate': self.error_count / max(self.request_count, 1) * 100,
            'avg_response_time': sum(times) / len(times),
            'p50_response_time': times[len(times) // 2],
            'p95_response_time': times[int(len(times) * 0.95)],
            'p99_response_time': times[int(len(times) * 0.99)]
        }

# ä¸šåŠ¡æŒ‡æ ‡æ”¶é›†å™¨
class BusinessMetricsCollector:
    def __init__(self, metrics_collector: MetricsCollector):
        self.metrics_collector = metrics_collector
        self.user_actions = defaultdict(int)
        self.conversion_funnel = defaultdict(int)
    
    def track_user_action(self, action: str, user_id: str = None):
        """è·Ÿè¸ªç”¨æˆ·è¡Œä¸º"""
        self.user_actions[action] += 1
        
        tags = {'action': action}
        if user_id:
            tags['user_id'] = user_id
        
        self.metrics_collector.record_metric('business.user_action', 1, tags)
    
    def track_conversion(self, step: str, user_id: str):
        """è·Ÿè¸ªè½¬åŒ–æ¼æ–—"""
        self.conversion_funnel[step] += 1
        
        tags = {'step': step, 'user_id': user_id}
        self.metrics_collector.record_metric('business.conversion', 1, tags)
    
    def get_business_summary(self) -> Dict:
        """è·å–ä¸šåŠ¡æŒ‡æ ‡æ‘˜è¦"""
        return {
            'user_actions': dict(self.user_actions),
            'conversion_funnel': dict(self.conversion_funnel)
        }

# ä½¿ç”¨ç¤ºä¾‹
def setup_monitoring():
    # åˆ›å»ºæŒ‡æ ‡æ”¶é›†å™¨
    collector = MetricsCollector(collection_interval=30)
    
    # åˆ›å»ºæ€§èƒ½ç›‘æ§å™¨
    perf_monitor = PerformanceMonitor(collector)
    
    # åˆ›å»ºä¸šåŠ¡æŒ‡æ ‡æ”¶é›†å™¨
    business_collector = BusinessMetricsCollector(collector)
    
    # æ·»åŠ è‡ªå®šä¹‰æŒ‡æ ‡æ”¶é›†å™¨
    def collect_database_metrics(collector):
        # æ¨¡æ‹Ÿæ•°æ®åº“è¿æ¥æ± æŒ‡æ ‡
        active_connections = 15
        max_connections = 100
        
        collector.record_metric('database.connections.active', active_connections)
        collector.record_metric('database.connections.usage', 
                              active_connections / max_connections * 100, 
                              {'unit': 'percent'})
    
    collector.add_custom_collector(collect_database_metrics)
    
    # å¯åŠ¨æ”¶é›†
    collector.start()
    
    return collector, perf_monitor, business_collector

# ç¤ºä¾‹åº”ç”¨
@perf_monitor.monitor_request('user_login')
def user_login(username: str, password: str):
    # æ¨¡æ‹Ÿç™»å½•é€»è¾‘
    time.sleep(0.1)  # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
    
    if username == "admin" and password == "wrong":
        raise ValueError("Invalid credentials")
    
    business_collector.track_user_action('login', username)
    return {"status": "success", "user": username}
```

## é“¾è·¯è¿½è¸ª

### âš¡ é«˜çº§é¢˜ç›®

#### 5. å¦‚ä½•å®ç°åˆ†å¸ƒå¼é“¾è·¯è¿½è¸ªç³»ç»Ÿï¼Ÿ

**ç­”æ¡ˆè¦ç‚¹ï¼š**
- **Trace ID**ï¼šå”¯ä¸€æ ‡è¯†ä¸€æ¬¡å®Œæ•´çš„è¯·æ±‚é“¾è·¯
- **Span ID**ï¼šæ ‡è¯†é“¾è·¯ä¸­çš„å•ä¸ªæ“ä½œ
- **ä¸Šä¸‹æ–‡ä¼ æ’­**ï¼šåœ¨æœåŠ¡é—´ä¼ é€’è¿½è¸ªä¿¡æ¯
- **é‡‡æ ·ç­–ç•¥**ï¼šæ§åˆ¶è¿½è¸ªæ•°æ®é‡

```mermaid
sequenceDiagram
    participant Client
    participant Gateway
    participant UserService
    participant OrderService
    participant Database
    
    Client->>Gateway: HTTP Request (Trace-ID: abc123)
    Gateway->>UserService: Get User (Trace-ID: abc123, Span-ID: span1)
    UserService->>Database: Query User (Trace-ID: abc123, Span-ID: span2)
    Database-->>UserService: User Data
    UserService-->>Gateway: User Info
    Gateway->>OrderService: Get Orders (Trace-ID: abc123, Span-ID: span3)
    OrderService->>Database: Query Orders (Trace-ID: abc123, Span-ID: span4)
    Database-->>OrderService: Order Data
    OrderService-->>Gateway: Order Info
    Gateway-->>Client: Response
```

```python
import uuid
import time
import threading
from contextlib import contextmanager
from dataclasses import dataclass, field
from typing import Dict, List, Optional, Any
import json

@dataclass
class Span:
    trace_id: str
    span_id: str
    parent_span_id: Optional[str]
    operation_name: str
    start_time: float
    end_time: Optional[float] = None
    tags: Dict[str, Any] = field(default_factory=dict)
    logs: List[Dict[str, Any]] = field(default_factory=list)
    status: str = "ok"  # ok, error
    
    def finish(self):
        """ç»“æŸ Span"""
        self.end_time = time.time()
    
    def set_tag(self, key: str, value: Any):
        """è®¾ç½®æ ‡ç­¾"""
        self.tags[key] = value
    
    def log(self, message: str, **kwargs):
        """æ·»åŠ æ—¥å¿—"""
        log_entry = {
            'timestamp': time.time(),
            'message': message,
            **kwargs
        }
        self.logs.append(log_entry)
    
    def set_error(self, error: Exception):
        """è®¾ç½®é”™è¯¯çŠ¶æ€"""
        self.status = "error"
        self.set_tag("error", True)
        self.set_tag("error.type", type(error).__name__)
        self.set_tag("error.message", str(error))
    
    def to_dict(self) -> Dict:
        """è½¬æ¢ä¸ºå­—å…¸"""
        return {
            'trace_id': self.trace_id,
            'span_id': self.span_id,
            'parent_span_id': self.parent_span_id,
            'operation_name': self.operation_name,
            'start_time': self.start_time,
            'end_time': self.end_time,
            'duration': (self.end_time - self.start_time) if self.end_time else None,
            'tags': self.tags,
            'logs': self.logs,
            'status': self.status
        }

class TraceContext:
    """è¿½è¸ªä¸Šä¸‹æ–‡"""
    def __init__(self):
        self._local = threading.local()
    
    def get_current_span(self) -> Optional[Span]:
        """è·å–å½“å‰ Span"""
        return getattr(self._local, 'current_span', None)
    
    def set_current_span(self, span: Optional[Span]):
        """è®¾ç½®å½“å‰ Span"""
        self._local.current_span = span
    
    def get_trace_id(self) -> Optional[str]:
        """è·å–å½“å‰ Trace ID"""
        span = self.get_current_span()
        return span.trace_id if span else None

class Tracer:
    def __init__(self, service_name: str, reporter=None):
        self.service_name = service_name
        self.reporter = reporter
        self.context = TraceContext()
        self.active_spans = {}
    
    def start_span(self, operation_name: str, 
                   parent_span: Optional[Span] = None,
                   trace_id: Optional[str] = None) -> Span:
        """å¼€å§‹ä¸€ä¸ªæ–°çš„ Span"""
        
        # ç¡®å®š trace_id
        if trace_id:
            current_trace_id = trace_id
        elif parent_span:
            current_trace_id = parent_span.trace_id
        else:
            current_span = self.context.get_current_span()
            current_trace_id = current_span.trace_id if current_span else str(uuid.uuid4())
        
        # åˆ›å»º Span
        span = Span(
            trace_id=current_trace_id,
            span_id=str(uuid.uuid4()),
            parent_span_id=parent_span.span_id if parent_span else None,
            operation_name=operation_name,
            start_time=time.time()
        )
        
        # è®¾ç½®æœåŠ¡æ ‡ç­¾
        span.set_tag("service.name", self.service_name)
        
        # å­˜å‚¨æ´»è·ƒ Span
        self.active_spans[span.span_id] = span
        
        return span
    
    def finish_span(self, span: Span):
        """ç»“æŸ Span"""
        span.finish()
        
        # ä»æ´»è·ƒ Span ä¸­ç§»é™¤
        self.active_spans.pop(span.span_id, None)
        
        # æŠ¥å‘Š Span
        if self.reporter:
            self.reporter.report(span)
    
    @contextmanager
    def span(self, operation_name: str, **tags):
        """Span ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
        parent_span = self.context.get_current_span()
        span = self.start_span(operation_name, parent_span)
        
        # è®¾ç½®æ ‡ç­¾
        for key, value in tags.items():
            span.set_tag(key, value)
        
        # è®¾ç½®ä¸ºå½“å‰ Span
        previous_span = self.context.get_current_span()
        self.context.set_current_span(span)
        
        try:
            yield span
        except Exception as e:
            span.set_error(e)
            raise
        finally:
            self.finish_span(span)
            self.context.set_current_span(previous_span)

class SpanReporter:
    """Span æŠ¥å‘Šå™¨"""
    def __init__(self, endpoint: str = None):
        self.endpoint = endpoint
        self.spans_buffer = []
        self.buffer_size = 100
    
    def report(self, span: Span):
        """æŠ¥å‘Š Span"""
        self.spans_buffer.append(span.to_dict())
        
        if len(self.spans_buffer) >= self.buffer_size:
            self._flush()
    
    def _flush(self):
        """åˆ·æ–°ç¼“å†²åŒº"""
        if not self.spans_buffer:
            return
        
        try:
            # è¿™é‡Œåº”è¯¥å‘é€åˆ°è¿½è¸ªç³»ç»Ÿï¼ˆå¦‚ Jaegerã€Zipkinï¼‰
            print(f"æŠ¥å‘Š {len(self.spans_buffer)} ä¸ª Span")
            for span_data in self.spans_buffer:
                print(json.dumps(span_data, indent=2))
            
            self.spans_buffer.clear()
            
        except Exception as e:
            print(f"æŠ¥å‘Š Span å¤±è´¥: {e}")

# HTTP è¿½è¸ªä¸­é—´ä»¶
class TracingMiddleware:
    def __init__(self, tracer: Tracer):
        self.tracer = tracer
    
    def __call__(self, request, response, next_handler):
        """å¤„ç† HTTP è¯·æ±‚è¿½è¸ª"""
        # ä»è¯·æ±‚å¤´ä¸­æå–è¿½è¸ªä¿¡æ¯
        trace_id = request.headers.get('X-Trace-ID')
        parent_span_id = request.headers.get('X-Parent-Span-ID')
        
        # åˆ›å»ºè¯·æ±‚ Span
        with self.tracer.span('http_request', 
                             method=request.method,
                             url=request.url,
                             user_agent=request.headers.get('User-Agent')) as span:
            
            # è®¾ç½®è¿½è¸ªå¤´
            response.headers['X-Trace-ID'] = span.trace_id
            response.headers['X-Span-ID'] = span.span_id
            
            try:
                result = next_handler(request, response)
                span.set_tag('http.status_code', response.status_code)
                return result
                
            except Exception as e:
                span.set_tag('http.status_code', 500)
                span.set_error(e)
                raise

# æ•°æ®åº“è¿½è¸ªè£…é¥°å™¨
def trace_database_operation(tracer: Tracer, operation_type: str):
    """æ•°æ®åº“æ“ä½œè¿½è¸ªè£…é¥°å™¨"""
    def decorator(func):
        def wrapper(*args, **kwargs):
            with tracer.span(f'db.{operation_type}',
                           db_type='postgresql',
                           db_operation=operation_type) as span:
                
                # è®°å½• SQL æŸ¥è¯¢ï¼ˆæ³¨æ„ä¸è¦è®°å½•æ•æ„Ÿä¿¡æ¯ï¼‰
                if 'query' in kwargs:
                    span.set_tag('db.statement', kwargs['query'][:100])  # æˆªæ–­é•¿æŸ¥è¯¢
                
                try:
                    result = func(*args, **kwargs)
                    span.set_tag('db.rows_affected', getattr(result, 'rowcount', 0))
                    return result
                    
                except Exception as e:
                    span.set_error(e)
                    raise
        
        return wrapper
    return decorator

# ä½¿ç”¨ç¤ºä¾‹
def setup_tracing():
    # åˆ›å»ºæŠ¥å‘Šå™¨
    reporter = SpanReporter()
    
    # åˆ›å»ºè¿½è¸ªå™¨
    tracer = Tracer('user-service', reporter)
    
    return tracer

# ç¤ºä¾‹æœåŠ¡
class UserService:
    def __init__(self, tracer: Tracer):
        self.tracer = tracer
    
    def get_user(self, user_id: int):
        """è·å–ç”¨æˆ·ä¿¡æ¯"""
        with self.tracer.span('get_user', user_id=user_id) as span:
            # æ¨¡æ‹Ÿæ•°æ®åº“æŸ¥è¯¢
            user_data = self._query_database(user_id)
            
            # æ¨¡æ‹Ÿç¼“å­˜æŸ¥è¯¢
            cached_data = self._query_cache(user_id)
            
            span.log("ç”¨æˆ·æ•°æ®è·å–å®Œæˆ", user_id=user_id)
            return user_data
    
    @trace_database_operation(tracer, 'select')
    def _query_database(self, user_id: int):
        """æŸ¥è¯¢æ•°æ®åº“"""
        time.sleep(0.05)  # æ¨¡æ‹Ÿæ•°æ®åº“å»¶è¿Ÿ
        return {"id": user_id, "name": f"User {user_id}"}
    
    def _query_cache(self, user_id: int):
        """æŸ¥è¯¢ç¼“å­˜"""
        with self.tracer.span('cache_query', 
                             cache_key=f'user:{user_id}') as span:
            time.sleep(0.01)  # æ¨¡æ‹Ÿç¼“å­˜å»¶è¿Ÿ
            span.set_tag('cache.hit', False)
            return None

# è·¨æœåŠ¡è°ƒç”¨ç¤ºä¾‹
class OrderService:
    def __init__(self, tracer: Tracer, user_service: UserService):
        self.tracer = tracer
        self.user_service = user_service
    
    def create_order(self, user_id: int, items: List[str]):
        """åˆ›å»ºè®¢å•"""
        with self.tracer.span('create_order', 
                             user_id=user_id,
                             item_count=len(items)) as span:
            
            # è·å–ç”¨æˆ·ä¿¡æ¯ï¼ˆè·¨æœåŠ¡è°ƒç”¨ï¼‰
            user = self.user_service.get_user(user_id)
            span.log("è·å–ç”¨æˆ·ä¿¡æ¯å®Œæˆ", user_name=user['name'])
            
            # åˆ›å»ºè®¢å•
            order_id = self._create_order_record(user_id, items)
            span.set_tag('order.id', order_id)
            
            return {"order_id": order_id, "user": user}
    
    def _create_order_record(self, user_id: int, items: List[str]):
        """åˆ›å»ºè®¢å•è®°å½•"""
        with self.tracer.span('db.insert_order') as span:
            time.sleep(0.1)  # æ¨¡æ‹Ÿæ•°æ®åº“å†™å…¥
            order_id = f"order_{int(time.time())}"
            span.set_tag('order.id', order_id)
            return order_id

# å®Œæ•´ç¤ºä¾‹
def tracing_example():
    tracer = setup_tracing()
    
    user_service = UserService(tracer)
    order_service = OrderService(tracer, user_service)
    
    # æ¨¡æ‹Ÿè¯·æ±‚å¤„ç†
    with tracer.span('handle_request', 
                     endpoint='/orders',
                     method='POST') as root_span:
        
        try:
            result = order_service.create_order(123, ['item1', 'item2'])
            root_span.set_tag('response.status', 'success')
            print("è®¢å•åˆ›å»ºæˆåŠŸ:", result)
            
        except Exception as e:
            root_span.set_error(e)
            print("è®¢å•åˆ›å»ºå¤±è´¥:", e)
```

## ğŸ”— ç›¸å…³é“¾æ¥

- [â† è¿”å›åç«¯é¢è¯•é¢˜ç›®å½•](./README.md)
- [æ€§èƒ½ä¼˜åŒ–é¢è¯•é¢˜](./performance-optimization.md)
- [åˆ†å¸ƒå¼ç³»ç»Ÿé¢è¯•é¢˜](./distributed-systems.md)
- [å¾®æœåŠ¡æ¶æ„é¢è¯•é¢˜](./microservices.md)

---

*æŒæ¡å…¨é¢çš„ç›‘æ§ä¸è°ƒè¯•æŠ€èƒ½ï¼Œæ„å»ºå¯è§‚æµ‹çš„ç³»ç»Ÿ* ğŸš€ 