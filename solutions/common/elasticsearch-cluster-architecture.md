# Elasticsearché›†ç¾¤æ¶æ„è®¾è®¡å®Œæ•´å®ç°

## ğŸ¯ è§£å†³æ–¹æ¡ˆæ¦‚è¿°

æ·±å…¥åˆ†æElasticsearché›†ç¾¤çš„æ¶æ„è®¾è®¡å’Œå®ç°æ–¹æ¡ˆï¼Œé€šè¿‡å®Œæ•´çš„é…ç½®å’Œä»£ç å±•ç¤ºå¤§è§„æ¨¡æœç´¢ç³»ç»Ÿçš„æ„å»ºæ–¹æ³•ã€‚

## ğŸ’¡ æ ¸å¿ƒé—®é¢˜åˆ†æ

### å¤§è§„æ¨¡æœç´¢ç³»ç»Ÿçš„æŠ€æœ¯æŒ‘æˆ˜
**ä¸šåŠ¡èƒŒæ™¯**ï¼šæ„å»ºæ”¯æŒPBçº§æ•°æ®å’Œäº¿çº§ç”¨æˆ·çš„æœç´¢æœåŠ¡
**æŠ€æœ¯éš¾ç‚¹**ï¼š
- é›†ç¾¤èŠ‚ç‚¹è§’è‰²çš„åˆç†åˆ†å·¥å’Œé…ç½®ä¼˜åŒ–
- åˆ†ç‰‡å’Œå‰¯æœ¬æ•°é‡çš„ç§‘å­¦è§„åˆ’å’ŒåŠ¨æ€è°ƒæ•´
- æ•°æ®åˆ†å¸ƒçš„å‡è¡¡æ€§å’ŒæŸ¥è¯¢è·¯ç”±çš„æ•ˆç‡
- æ•…éšœæ¢å¤çš„å¿«é€Ÿæ€§å’Œæ•°æ®ä¸€è‡´æ€§ä¿éšœ

## ğŸ“ é¢˜ç›®1ï¼šElasticsearché›†ç¾¤æ¶æ„è®¾è®¡

### è§£å†³æ–¹æ¡ˆæ€è·¯åˆ†æ

#### 1. é›†ç¾¤èŠ‚ç‚¹è§’è‰²è®¾è®¡ç­–ç•¥
**ä¸ºä»€ä¹ˆéœ€è¦èŠ‚ç‚¹è§’è‰²åˆ†ç¦»ï¼Ÿ**
- ä¸»èŠ‚ç‚¹ä¸“æ³¨é›†ç¾¤çŠ¶æ€ç®¡ç†ï¼Œé¿å…æ•°æ®å¤„ç†å¹²æ‰°
- æ•°æ®èŠ‚ç‚¹ä¼˜åŒ–å­˜å‚¨å’Œè®¡ç®—èµ„æºé…ç½®
- åè°ƒèŠ‚ç‚¹å‡è½»æ•°æ®èŠ‚ç‚¹çš„æŸ¥è¯¢åè°ƒè´Ÿæ‹…
- ä¸“ç”¨èŠ‚ç‚¹æé«˜é›†ç¾¤çš„ç¨³å®šæ€§å’Œå¯æ‰©å±•æ€§

#### 2. åˆ†ç‰‡ç­–ç•¥è®¾è®¡åŸç†
**åˆ†ç‰‡è§„åˆ’ç­–ç•¥**ï¼š
- åŸºäºæ•°æ®é‡å’ŒæŸ¥è¯¢æ¨¡å¼çš„åˆ†ç‰‡å¤§å°è®¡ç®—
- è€ƒè™‘ç¡¬ä»¶èµ„æºå’Œç½‘ç»œå¸¦å®½çš„åˆ†ç‰‡åˆ†å¸ƒ
- æ”¯æŒæ°´å¹³æ‰©å±•çš„åˆ†ç‰‡æ•°é‡é¢„ç•™
- åˆ†ç‰‡é‡å¹³è¡¡çš„è‡ªåŠ¨åŒ–å’Œæ€§èƒ½ä¼˜åŒ–

#### 3. é«˜å¯ç”¨ä¿éšœæœºåˆ¶
**å¯ç”¨æ€§è®¾è®¡è¦ç‚¹**ï¼š
- å¤šå‰¯æœ¬çš„æ•°æ®å†—ä½™å’Œæ•…éšœåˆ‡æ¢
- è·¨æœºæ¶/æ•°æ®ä¸­å¿ƒçš„èŠ‚ç‚¹åˆ†å¸ƒ
- è„‘è£‚é—®é¢˜çš„é¢„é˜²å’Œæ£€æµ‹æœºåˆ¶
- æ»šåŠ¨å‡çº§å’Œé›¶åœæœºç»´æŠ¤ç­–ç•¥

### ä»£ç å®ç°è¦ç‚¹

#### Elasticsearché›†ç¾¤é…ç½®å®ç°
é€šè¿‡å®Œæ•´çš„é…ç½®å’Œç®¡ç†ä»£ç å±•ç¤ºé›†ç¾¤æ¶æ„

```yaml
# elasticsearch.yml - ä¸»èŠ‚ç‚¹é…ç½®
cluster.name: production-search-cluster
node.name: master-node-${HOSTNAME}

# èŠ‚ç‚¹è§’è‰²é…ç½®
node.roles: [ master ]

# ç½‘ç»œé…ç½®
network.host: 0.0.0.0
http.port: 9200
transport.port: 9300

# é›†ç¾¤å‘ç°é…ç½®
discovery.seed_hosts: 
  - master-node-1:9300
  - master-node-2:9300
  - master-node-3:9300

cluster.initial_master_nodes:
  - master-node-1
  - master-node-2
  - master-node-3

# é›†ç¾¤è®¾ç½®
cluster.routing.allocation.awareness.attributes: rack_id,zone
cluster.routing.allocation.awareness.force.zone.values: zone-1,zone-2,zone-3

# é˜²æ­¢è„‘è£‚
discovery.zen.minimum_master_nodes: 2
gateway.expected_master_nodes: 3
gateway.expected_data_nodes: 6
gateway.recover_after_master_nodes: 2
gateway.recover_after_data_nodes: 4

# JVMé…ç½®
bootstrap.memory_lock: true

# ç´¢å¼•é…ç½®
action.auto_create_index: false
action.destructive_requires_name: true

---
# elasticsearch.yml - æ•°æ®èŠ‚ç‚¹é…ç½®
cluster.name: production-search-cluster
node.name: data-node-${HOSTNAME}

# èŠ‚ç‚¹è§’è‰²é…ç½®
node.roles: [ data, ingest ]

# ç½‘ç»œé…ç½®
network.host: 0.0.0.0
http.port: 9200
transport.port: 9300

# é›†ç¾¤å‘ç°é…ç½®
discovery.seed_hosts: 
  - master-node-1:9300
  - master-node-2:9300
  - master-node-3:9300

# èŠ‚ç‚¹å±æ€§
node.attr.rack_id: ${RACK_ID}
node.attr.zone: ${ZONE}

# æ•°æ®è·¯å¾„é…ç½®
path.data: 
  - /data1/elasticsearch
  - /data2/elasticsearch
path.logs: /var/log/elasticsearch

# å†…å­˜å’Œå­˜å‚¨é…ç½®
indices.memory.index_buffer_size: 30%
indices.memory.min_index_buffer_size: 96mb

# çº¿ç¨‹æ± é…ç½®
thread_pool.write.queue_size: 1000
thread_pool.search.queue_size: 1000

---
# elasticsearch.yml - åè°ƒèŠ‚ç‚¹é…ç½®
cluster.name: production-search-cluster
node.name: coordinating-node-${HOSTNAME}

# èŠ‚ç‚¹è§’è‰²é…ç½®ï¼ˆä»…åè°ƒï¼Œä¸å­˜å‚¨æ•°æ®ï¼‰
node.roles: []

# ç½‘ç»œé…ç½®
network.host: 0.0.0.0
http.port: 9200
transport.port: 9300

# é›†ç¾¤å‘ç°é…ç½®
discovery.seed_hosts: 
  - master-node-1:9300
  - master-node-2:9300
  - master-node-3:9300

# åè°ƒèŠ‚ç‚¹ä¼˜åŒ–
search.remote.connect: false
```

```python
# cluster_manager.py - é›†ç¾¤ç®¡ç†å·¥å…·
import json
import requests
import time
from typing import Dict, List, Any, Optional
from datetime import datetime
import logging

logger = logging.getLogger(__name__)

class ElasticsearchClusterManager:
    """Elasticsearché›†ç¾¤ç®¡ç†ç±»"""
    
    def __init__(self, hosts: List[str], auth: Optional[tuple] = None):
        self.hosts = hosts
        self.auth = auth
        self.session = requests.Session()
        if auth:
            self.session.auth = auth
    
    def get_cluster_health(self) -> Dict[str, Any]:
        """è·å–é›†ç¾¤å¥åº·çŠ¶æ€"""
        try:
            response = self.session.get(f"{self.hosts[0]}/_cluster/health")
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Failed to get cluster health: {e}")
            raise
    
    def get_cluster_stats(self) -> Dict[str, Any]:
        """è·å–é›†ç¾¤ç»Ÿè®¡ä¿¡æ¯"""
        try:
            response = self.session.get(f"{self.hosts[0]}/_cluster/stats")
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Failed to get cluster stats: {e}")
            raise
    
    def get_nodes_info(self) -> Dict[str, Any]:
        """è·å–èŠ‚ç‚¹ä¿¡æ¯"""
        try:
            response = self.session.get(f"{self.hosts[0]}/_nodes")
            response.raise_for_status()
            return response.json()
        except Exception as e:
            logger.error(f"Failed to get nodes info: {e}")
            raise
    
    def create_index_template(self, name: str, template: Dict[str, Any]):
        """åˆ›å»ºç´¢å¼•æ¨¡æ¿"""
        try:
            response = self.session.put(
                f"{self.hosts[0]}/_index_template/{name}",
                json=template,
                headers={'Content-Type': 'application/json'}
            )
            response.raise_for_status()
            logger.info(f"Index template '{name}' created successfully")
            return response.json()
        except Exception as e:
            logger.error(f"Failed to create index template '{name}': {e}")
            raise
    
    def setup_index_lifecycle_policy(self, policy_name: str, policy: Dict[str, Any]):
        """è®¾ç½®ç´¢å¼•ç”Ÿå‘½å‘¨æœŸç­–ç•¥"""
        try:
            response = self.session.put(
                f"{self.hosts[0]}/_ilm/policy/{policy_name}",
                json=policy,
                headers={'Content-Type': 'application/json'}
            )
            response.raise_for_status()
            logger.info(f"ILM policy '{policy_name}' created successfully")
            return response.json()
        except Exception as e:
            logger.error(f"Failed to create ILM policy '{policy_name}': {e}")
            raise
    
    def configure_cluster_settings(self, settings: Dict[str, Any]):
        """é…ç½®é›†ç¾¤è®¾ç½®"""
        try:
            response = self.session.put(
                f"{self.hosts[0]}/_cluster/settings",
                json=settings,
                headers={'Content-Type': 'application/json'}
            )
            response.raise_for_status()
            logger.info("Cluster settings updated successfully")
            return response.json()
        except Exception as e:
            logger.error(f"Failed to update cluster settings: {e}")
            raise
    
    def monitor_cluster_status(self, interval: int = 60):
        """ç›‘æ§é›†ç¾¤çŠ¶æ€"""
        logger.info(f"Starting cluster monitoring (interval: {interval}s)")
        
        while True:
            try:
                health = self.get_cluster_health()
                stats = self.get_cluster_stats()
                
                # æ£€æŸ¥å…³é”®æŒ‡æ ‡
                status = health.get('status', 'unknown')
                active_shards = health.get('active_shards', 0)
                relocating_shards = health.get('relocating_shards', 0)
                unassigned_shards = health.get('unassigned_shards', 0)
                
                # è®°å½•çŠ¶æ€
                logger.info(f"Cluster Status: {status}")
                logger.info(f"Active Shards: {active_shards}")
                logger.info(f"Relocating Shards: {relocating_shards}")
                logger.info(f"Unassigned Shards: {unassigned_shards}")
                
                # æ£€æŸ¥è­¦å‘Šæ¡ä»¶
                if status == 'red':
                    logger.error("ALERT: Cluster status is RED!")
                elif status == 'yellow':
                    logger.warning("WARNING: Cluster status is YELLOW")
                
                if unassigned_shards > 0:
                    logger.warning(f"WARNING: {unassigned_shards} unassigned shards")
                
                time.sleep(interval)
                
            except KeyboardInterrupt:
                logger.info("Monitoring stopped by user")
                break
            except Exception as e:
                logger.error(f"Monitoring error: {e}")
                time.sleep(interval)

def setup_production_cluster():
    """ç”Ÿäº§ç¯å¢ƒé›†ç¾¤åˆå§‹åŒ–é…ç½®"""
    
    # é›†ç¾¤ç®¡ç†å™¨
    cluster_manager = ElasticsearchClusterManager(
        hosts=["http://coordinating-node-1:9200", 
               "http://coordinating-node-2:9200"]
    )
    
    # 1. é…ç½®é›†ç¾¤è®¾ç½®
    cluster_settings = {
        "persistent": {
            # åˆ†ç‰‡åˆ†é…è®¾ç½®
            "cluster.routing.allocation.disk.threshold.enabled": True,
            "cluster.routing.allocation.disk.watermark.low": "85%",
            "cluster.routing.allocation.disk.watermark.high": "90%",
            "cluster.routing.allocation.disk.watermark.flood_stage": "95%",
            
            # æ¢å¤è®¾ç½®
            "cluster.routing.allocation.node_concurrent_recoveries": 2,
            "cluster.routing.allocation.cluster_concurrent_rebalance": 2,
            "indices.recovery.max_bytes_per_sec": "100mb",
            
            # æœç´¢è®¾ç½®
            "search.max_buckets": 100000,
            "search.allow_expensive_queries": False,
            
            # ç´¢å¼•è®¾ç½®
            "action.auto_create_index": False,
            "action.destructive_requires_name": True
        }
    }
    
    cluster_manager.configure_cluster_settings(cluster_settings)
    
    # 2. åˆ›å»ºç´¢å¼•æ¨¡æ¿
    log_template = {
        "index_patterns": ["logs-*"],
        "template": {
            "settings": {
                "number_of_shards": 1,
                "number_of_replicas": 1,
                "refresh_interval": "30s",
                "index.codec": "best_compression",
                "index.lifecycle.name": "logs-policy",
                "index.lifecycle.rollover_alias": "logs"
            },
            "mappings": {
                "properties": {
                    "@timestamp": {
                        "type": "date",
                        "format": "strict_date_optional_time||epoch_millis"
                    },
                    "level": {
                        "type": "keyword"
                    },
                    "service": {
                        "type": "keyword"
                    },
                    "message": {
                        "type": "text",
                        "analyzer": "standard"
                    },
                    "host": {
                        "type": "keyword"
                    },
                    "ip": {
                        "type": "ip"
                    },
                    "user_id": {
                        "type": "keyword"
                    },
                    "request_id": {
                        "type": "keyword"
                    },
                    "duration": {
                        "type": "long"
                    },
                    "status_code": {
                        "type": "integer"
                    }
                }
            }
        },
        "priority": 100
    }
    
    cluster_manager.create_index_template("logs-template", log_template)
    
    # 3. è®¾ç½®ç´¢å¼•ç”Ÿå‘½å‘¨æœŸç­–ç•¥
    logs_ilm_policy = {
        "policy": {
            "phases": {
                "hot": {
                    "actions": {
                        "rollover": {
                            "max_size": "10GB",
                            "max_age": "1d",
                            "max_docs": 10000000
                        },
                        "set_priority": {
                            "priority": 100
                        }
                    }
                },
                "warm": {
                    "min_age": "1d",
                    "actions": {
                        "allocate": {
                            "number_of_replicas": 0
                        },
                        "forcemerge": {
                            "max_num_segments": 1
                        },
                        "set_priority": {
                            "priority": 50
                        }
                    }
                },
                "cold": {
                    "min_age": "7d",
                    "actions": {
                        "allocate": {
                            "number_of_replicas": 0,
                            "include": {
                                "node_type": "cold"
                            }
                        },
                        "set_priority": {
                            "priority": 10
                        }
                    }
                },
                "delete": {
                    "min_age": "30d",
                    "actions": {
                        "delete": {}
                    }
                }
            }
        }
    }
    
    cluster_manager.setup_index_lifecycle_policy("logs-policy", logs_ilm_policy)
    
    # 4. åˆ›å»ºç”¨æˆ·æ•°æ®ç´¢å¼•æ¨¡æ¿
    user_template = {
        "index_patterns": ["users-*"],
        "template": {
            "settings": {
                "number_of_shards": 3,
                "number_of_replicas": 2,
                "refresh_interval": "1s",
                "index.max_result_window": 50000
            },
            "mappings": {
                "properties": {
                    "user_id": {
                        "type": "keyword"
                    },
                    "username": {
                        "type": "text",
                        "fields": {
                            "keyword": {
                                "type": "keyword"
                            }
                        }
                    },
                    "email": {
                        "type": "keyword"
                    },
                    "profile": {
                        "type": "object",
                        "properties": {
                            "age": {"type": "integer"},
                            "gender": {"type": "keyword"},
                            "location": {"type": "geo_point"},
                            "interests": {"type": "keyword"}
                        }
                    },
                    "created_at": {
                        "type": "date"
                    },
                    "updated_at": {
                        "type": "date"
                    },
                    "status": {
                        "type": "keyword"
                    }
                }
            }
        },
        "priority": 200
    }
    
    cluster_manager.create_index_template("users-template", user_template)
    
    print("Production cluster setup completed successfully!")
    
    return cluster_manager

# é›†ç¾¤ç›‘æ§å’Œç®¡ç†è„šæœ¬
class ClusterHealthMonitor:
    """é›†ç¾¤å¥åº·ç›‘æ§ç±»"""
    
    def __init__(self, cluster_manager: ElasticsearchClusterManager):
        self.cluster_manager = cluster_manager
        self.alerts = []
    
    def check_cluster_health(self) -> Dict[str, Any]:
        """æ£€æŸ¥é›†ç¾¤å¥åº·çŠ¶æ€"""
        health = self.cluster_manager.get_cluster_health()
        stats = self.cluster_manager.get_cluster_stats()
        
        issues = []
        
        # æ£€æŸ¥é›†ç¾¤çŠ¶æ€
        if health.get('status') == 'red':
            issues.append({
                'severity': 'critical',
                'message': 'Cluster status is RED - some primary shards are not allocated'
            })
        elif health.get('status') == 'yellow':
            issues.append({
                'severity': 'warning',
                'message': 'Cluster status is YELLOW - some replica shards are not allocated'
            })
        
        # æ£€æŸ¥æœªåˆ†é…åˆ†ç‰‡
        unassigned_shards = health.get('unassigned_shards', 0)
        if unassigned_shards > 0:
            issues.append({
                'severity': 'warning',
                'message': f'{unassigned_shards} shards are unassigned'
            })
        
        # æ£€æŸ¥ç£ç›˜ä½¿ç”¨ç‡
        nodes = self.cluster_manager.get_nodes_info()
        for node_id, node_info in nodes.get('nodes', {}).items():
            fs_info = node_info.get('fs', {})
            if fs_info:
                total_bytes = fs_info.get('total', {}).get('total_in_bytes', 0)
                available_bytes = fs_info.get('total', {}).get('available_in_bytes', 0)
                
                if total_bytes > 0:
                    usage_percent = (total_bytes - available_bytes) / total_bytes * 100
                    if usage_percent > 90:
                        issues.append({
                            'severity': 'critical',
                            'message': f'Node {node_info.get("name", node_id)} disk usage: {usage_percent:.1f}%'
                        })
                    elif usage_percent > 85:
                        issues.append({
                            'severity': 'warning',
                            'message': f'Node {node_info.get("name", node_id)} disk usage: {usage_percent:.1f}%'
                        })
        
        return {
            'timestamp': datetime.now().isoformat(),
            'cluster_health': health,
            'cluster_stats': stats,
            'issues': issues
        }
    
    def generate_health_report(self) -> str:
        """ç”Ÿæˆå¥åº·æŠ¥å‘Š"""
        health_data = self.check_cluster_health()
        
        report = f"""
Elasticsearch Cluster Health Report
Generated: {health_data['timestamp']}

=== Cluster Overview ===
Status: {health_data['cluster_health'].get('status', 'unknown').upper()}
Nodes: {health_data['cluster_health'].get('number_of_nodes', 0)}
Data Nodes: {health_data['cluster_health'].get('number_of_data_nodes', 0)}
Active Shards: {health_data['cluster_health'].get('active_shards', 0)}
Relocating Shards: {health_data['cluster_health'].get('relocating_shards', 0)}
Unassigned Shards: {health_data['cluster_health'].get('unassigned_shards', 0)}

=== Issues Detected ===
"""
        
        if health_data['issues']:
            for issue in health_data['issues']:
                report += f"[{issue['severity'].upper()}] {issue['message']}\n"
        else:
            report += "No issues detected.\n"
        
        return report

if __name__ == "__main__":
    # åˆå§‹åŒ–ç”Ÿäº§ç¯å¢ƒé›†ç¾¤
    cluster_manager = setup_production_cluster()
    
    # åˆ›å»ºå¥åº·ç›‘æ§å™¨
    health_monitor = ClusterHealthMonitor(cluster_manager)
    
    # ç”Ÿæˆå¥åº·æŠ¥å‘Š
    report = health_monitor.generate_health_report()
    print(report)
    
    # å¯åŠ¨ç›‘æ§ï¼ˆå¯é€‰ï¼‰
    # cluster_manager.monitor_cluster_status(interval=60)
```

```dockerfile
# Dockerfile for Elasticsearch node
FROM docker.elastic.co/elasticsearch/elasticsearch:8.11.0

# å®‰è£…æ’ä»¶
RUN bin/elasticsearch-plugin install analysis-icu
RUN bin/elasticsearch-plugin install analysis-smartcn

# å¤åˆ¶é…ç½®æ–‡ä»¶
COPY elasticsearch.yml /usr/share/elasticsearch/config/
COPY jvm.options /usr/share/elasticsearch/config/

# è®¾ç½®ç¯å¢ƒå˜é‡
ENV ES_JAVA_OPTS="-Xms4g -Xmx4g"
ENV discovery.type=zen

# å¥åº·æ£€æŸ¥
HEALTHCHECK --interval=30s --timeout=10s --start-period=60s --retries=3 \
  CMD curl -f http://localhost:9200/_cluster/health || exit 1

EXPOSE 9200 9300
```

```yaml
# docker-compose.yml for Elasticsearch cluster
version: '3.8'

services:
  # Master nodes
  es-master-1:
    build: .
    container_name: es-master-1
    environment:
      - node.name=es-master-1
      - cluster.name=production-cluster
      - node.roles=master
      - discovery.seed_hosts=es-master-1,es-master-2,es-master-3
      - cluster.initial_master_nodes=es-master-1,es-master-2,es-master-3
      - HOSTNAME=es-master-1
    volumes:
      - master1_data:/usr/share/elasticsearch/data
      - ./config/master.yml:/usr/share/elasticsearch/config/elasticsearch.yml
    networks:
      - elastic

  es-master-2:
    build: .
    container_name: es-master-2
    environment:
      - node.name=es-master-2
      - cluster.name=production-cluster
      - node.roles=master
      - discovery.seed_hosts=es-master-1,es-master-2,es-master-3
      - cluster.initial_master_nodes=es-master-1,es-master-2,es-master-3
      - HOSTNAME=es-master-2
    volumes:
      - master2_data:/usr/share/elasticsearch/data
      - ./config/master.yml:/usr/share/elasticsearch/config/elasticsearch.yml
    networks:
      - elastic

  es-master-3:
    build: .
    container_name: es-master-3
    environment:
      - node.name=es-master-3
      - cluster.name=production-cluster
      - node.roles=master
      - discovery.seed_hosts=es-master-1,es-master-2,es-master-3
      - cluster.initial_master_nodes=es-master-1,es-master-2,es-master-3
      - HOSTNAME=es-master-3
    volumes:
      - master3_data:/usr/share/elasticsearch/data
      - ./config/master.yml:/usr/share/elasticsearch/config/elasticsearch.yml
    networks:
      - elastic

  # Data nodes
  es-data-1:
    build: .
    container_name: es-data-1
    environment:
      - node.name=es-data-1
      - cluster.name=production-cluster
      - node.roles=data,ingest
      - discovery.seed_hosts=es-master-1,es-master-2,es-master-3
      - node.attr.rack_id=rack-1
      - node.attr.zone=zone-1
      - HOSTNAME=es-data-1
      - RACK_ID=rack-1
      - ZONE=zone-1
    volumes:
      - data1_data:/usr/share/elasticsearch/data
      - ./config/data.yml:/usr/share/elasticsearch/config/elasticsearch.yml
    networks:
      - elastic
    depends_on:
      - es-master-1
      - es-master-2
      - es-master-3

  # Coordinating nodes
  es-coord-1:
    build: .
    container_name: es-coord-1
    environment:
      - node.name=es-coord-1
      - cluster.name=production-cluster
      - node.roles=[]
      - discovery.seed_hosts=es-master-1,es-master-2,es-master-3
      - HOSTNAME=es-coord-1
    ports:
      - "9200:9200"
    volumes:
      - ./config/coordinating.yml:/usr/share/elasticsearch/config/elasticsearch.yml
    networks:
      - elastic
    depends_on:
      - es-master-1
      - es-data-1

volumes:
  master1_data:
  master2_data:
  master3_data:
  data1_data:

networks:
  elastic:
    driver: bridge
```

## ğŸ¯ é¢è¯•è¦ç‚¹æ€»ç»“

### æŠ€æœ¯æ·±åº¦ä½“ç°
- **èŠ‚ç‚¹è§’è‰²åˆ†ç¦»**ï¼šä¸»èŠ‚ç‚¹ã€æ•°æ®èŠ‚ç‚¹ã€åè°ƒèŠ‚ç‚¹çš„èŒè´£åˆ’åˆ†å’Œä¼˜åŒ–é…ç½®
- **åˆ†ç‰‡ç­–ç•¥**ï¼šåŸºäºæ•°æ®é‡å’ŒæŸ¥è¯¢æ¨¡å¼çš„åˆ†ç‰‡å¤§å°å’Œæ•°é‡è®¡ç®—
- **é«˜å¯ç”¨è®¾è®¡**ï¼šå¤šå‰¯æœ¬ã€è·¨æœºæ¶éƒ¨ç½²ã€è„‘è£‚é¢„é˜²çš„å®Œæ•´æ–¹æ¡ˆ
- **æ€§èƒ½ä¼˜åŒ–**ï¼šJVMé…ç½®ã€çº¿ç¨‹æ± è°ƒä¼˜ã€ç´¢å¼•ç”Ÿå‘½å‘¨æœŸç®¡ç†

### ç”Ÿäº§å®è·µç»éªŒ
- **ç›‘æ§ä½“ç³»**ï¼šé›†ç¾¤å¥åº·ã€èŠ‚ç‚¹çŠ¶æ€ã€æ€§èƒ½æŒ‡æ ‡çš„å…¨æ–¹ä½ç›‘æ§
- **è¿ç»´è‡ªåŠ¨åŒ–**ï¼šç´¢å¼•æ¨¡æ¿ã€ILMç­–ç•¥ã€é…ç½®ç®¡ç†çš„è‡ªåŠ¨åŒ–
- **æ•…éšœå¤„ç†**ï¼šå¸¸è§é—®é¢˜çš„è¯Šæ–­å’Œè§£å†³æ–¹æ¡ˆ
- **å®¹é‡è§„åˆ’**ï¼šåŸºäºä¸šåŠ¡å¢é•¿çš„é›†ç¾¤æ‰©å®¹ç­–ç•¥

### é¢è¯•å›ç­”è¦ç‚¹
- **æ¶æ„è®¾è®¡**ï¼šä»ä¸šåŠ¡éœ€æ±‚åˆ°æŠ€æœ¯æ¶æ„çš„å®Œæ•´è®¾è®¡æ€è·¯
- **æ€§èƒ½è°ƒä¼˜**ï¼šJVMã€æ“ä½œç³»ç»Ÿã€ç½‘ç»œç­‰å¤šå±‚æ¬¡çš„ä¼˜åŒ–ç­–ç•¥
- **è¿ç»´ç»éªŒ**ï¼šç”Ÿäº§ç¯å¢ƒçš„éƒ¨ç½²ã€ç›‘æ§ã€æ•…éšœå¤„ç†å®è·µ
- **æ‰©å±•æ€§**ï¼šæ”¯æŒPBçº§æ•°æ®å’Œæ°´å¹³æ‰©å±•çš„æ¶æ„è®¾è®¡

[â† è¿”å›æœç´¢å¼•æ“é¢è¯•é¢˜](../../questions/backend/search-engines.md) 