# é«˜æ€§èƒ½æ—¥å¿—ç³»ç»Ÿå®ç°

[â† è¿”å›ç›‘æ§ä¸è°ƒè¯•é¢è¯•é¢˜](../../questions/backend/monitoring-debugging.md)

## ğŸ¯ è§£å†³æ–¹æ¡ˆæ¦‚è¿°

é«˜æ€§èƒ½æ—¥å¿—ç³»ç»Ÿæ˜¯ç°ä»£åº”ç”¨çš„é‡è¦åŸºç¡€è®¾æ–½ï¼Œéœ€è¦åœ¨ä¿è¯ä¸šåŠ¡æ€§èƒ½çš„å‰æä¸‹ï¼Œæä¾›ä¸°å¯Œçš„è¯Šæ–­ä¿¡æ¯å’Œå¼ºå¤§çš„åˆ†æèƒ½åŠ›ã€‚æœ¬è§£å†³æ–¹æ¡ˆæ·±å…¥åˆ†ææ—¥å¿—ç³»ç»Ÿçš„è®¾è®¡åŸç†ï¼ŒåŒ…æ‹¬å¼‚æ­¥å†™å…¥ã€ç»“æ„åŒ–æ—¥å¿—ã€å®‰å…¨è„±æ•ç­‰æ ¸å¿ƒæŠ€æœ¯ï¼Œå¹¶æä¾›ç”Ÿäº§çº§çš„å®ç°æ–¹æ¡ˆã€‚

## ğŸ’¡ æ ¸å¿ƒé—®é¢˜åˆ†æ

### é«˜å¹¶å‘ç³»ç»Ÿçš„æ—¥å¿—æŒ‘æˆ˜

**ä¸šåŠ¡èƒŒæ™¯**ï¼šåœ¨é«˜å¹¶å‘äº’è”ç½‘åº”ç”¨ä¸­ï¼Œæ—¥å¿—ç³»ç»Ÿé¢ä¸´ä»¥ä¸‹æŒ‘æˆ˜ï¼š
- æ¯ç§’æ•°ä¸‡æ¬¡çš„æ—¥å¿—å†™å…¥è¯·æ±‚
- æ—¥å¿—å†™å…¥ä¸èƒ½å½±å“ä¸šåŠ¡å“åº”æ—¶é—´
- éœ€è¦æ”¯æŒç»“æ„åŒ–æŸ¥è¯¢å’Œåˆ†æ
- å¿…é¡»ä¿è¯æ—¥å¿—æ•°æ®çš„å®Œæ•´æ€§å’Œå®‰å…¨æ€§

**æŠ€æœ¯éš¾ç‚¹**ï¼š
- å¦‚ä½•å®ç°é«˜æ€§èƒ½çš„å¼‚æ­¥å†™å…¥æœºåˆ¶
- å¦‚ä½•è®¾è®¡å¯æ‰©å±•çš„æ—¥å¿—å­˜å‚¨æ¶æ„
- å¦‚ä½•å®ç°æ•æ„Ÿä¿¡æ¯çš„è‡ªåŠ¨è„±æ•
- å¦‚ä½•ä¿è¯æ—¥å¿—ç³»ç»Ÿçš„é«˜å¯ç”¨æ€§

## ğŸ“ é¢˜ç›®1ï¼šé«˜æ€§èƒ½ç»“æ„åŒ–æ—¥å¿—ç³»ç»Ÿè®¾è®¡

### è§£å†³æ–¹æ¡ˆæ€è·¯åˆ†æ

#### 1. å¼‚æ­¥å†™å…¥æ¶æ„ç­–ç•¥

**ä¸ºä»€ä¹ˆé€‰æ‹©å¼‚æ­¥å†™å…¥ï¼Ÿ**

- **æ€§èƒ½ä¼˜åŠ¿**ï¼š
  - ä¸šåŠ¡çº¿ç¨‹ä¸é˜»å¡ï¼Œå“åº”æ—¶é—´ç¨³å®š
  - æ‰¹é‡å†™å…¥å‡å°‘I/Oæ“ä½œæ¬¡æ•°
  - å†…å­˜ç¼“å†²æé«˜å†™å…¥ååé‡

- **å®ç°åŸç†**ï¼š
  - ç”Ÿäº§è€…-æ¶ˆè´¹è€…æ¨¡å¼çš„é˜Ÿåˆ—è®¾è®¡
  - å¤šçº§ç¼“å†²å’Œæ‰¹é‡åˆ·ç›˜æœºåˆ¶
  - èƒŒå‹æ§åˆ¶é˜²æ­¢å†…å­˜æº¢å‡º

#### 2. ç»“æ„åŒ–æ—¥å¿—è®¾è®¡åŸç†

**JSONæ ¼å¼æ ‡å‡†åŒ–**ï¼š

- **å­—æ®µè§„èŒƒ**ï¼š
  - æ—¶é—´æˆ³ï¼šISO 8601æ ¼å¼
  - æ—¥å¿—çº§åˆ«ï¼šç»Ÿä¸€æšä¸¾å€¼
  - æœåŠ¡æ ‡è¯†ï¼šserviceã€versionã€instance
  - è¿½è¸ªä¿¡æ¯ï¼štrace_idã€span_id
  - ä¸šåŠ¡æ•°æ®ï¼šè‡ªå®šä¹‰å­—æ®µ

**ç´¢å¼•ä¼˜åŒ–ç­–ç•¥**ï¼š
- æ—¶é—´ç»´åº¦åˆ†ç‰‡å­˜å‚¨
- çƒ­ç‚¹å­—æ®µå»ºç«‹ç´¢å¼•
- å†·çƒ­æ•°æ®åˆ†å±‚å­˜å‚¨

#### 3. å®‰å…¨è„±æ•æœºåˆ¶è®¾è®¡æ€è·¯

**æ•æ„Ÿä¿¡æ¯è¯†åˆ«**ï¼š

- **è§„åˆ™å¼•æ“**ï¼š
  - æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…
  - å­—æ®µåç§°è¯†åˆ«
  - æ•°æ®æ ¼å¼æ£€æµ‹
  - æœºå™¨å­¦ä¹ è¾…åŠ©è¯†åˆ«

**è„±æ•ç­–ç•¥**ï¼š
- æ©ç å¤„ç†ï¼šä¿ç•™éƒ¨åˆ†ä¿¡æ¯
- å“ˆå¸Œå¤„ç†ï¼šä¿æŒå”¯ä¸€æ€§
- åŠ å¯†å­˜å‚¨ï¼šå¯é€†è„±æ•
- å®Œå…¨åˆ é™¤ï¼šé«˜æ•æ„Ÿä¿¡æ¯

### ä»£ç å®ç°è¦ç‚¹

#### é«˜æ€§èƒ½å¼‚æ­¥æ—¥å¿—å™¨å®ç°

```python
"""
é«˜æ€§èƒ½å¼‚æ­¥æ—¥å¿—ç³»ç»Ÿå®ç°

è®¾è®¡åŸç†ï¼š
1. æ— é”é˜Ÿåˆ—ï¼šä½¿ç”¨ring bufferå‡å°‘é”ç«äº‰
2. æ‰¹é‡å†™å…¥ï¼šèšåˆå¤šæ¡æ—¥å¿—å‡å°‘I/O
3. å†…å­˜æ˜ å°„ï¼šä½¿ç”¨mmapæé«˜æ–‡ä»¶å†™å…¥æ€§èƒ½
4. å¤šçº§ç¼“å†²ï¼šL1å†…å­˜ç¼“å†² + L2ç£ç›˜ç¼“å†²
"""

import asyncio
import json
import mmap
import os
import threading
import time
from collections import deque
from concurrent.futures import ThreadPoolExecutor
from dataclasses import dataclass, asdict
from datetime import datetime
from enum import Enum
from pathlib import Path
from queue import Queue, Empty
from typing import Dict, Any, Optional, List
import uuid
import re
import hashlib

class LogLevel(Enum):
    TRACE = 10
    DEBUG = 20
    INFO = 30
    WARN = 40
    ERROR = 50
    FATAL = 60

@dataclass
class LogRecord:
    timestamp: str
    level: str
    service: str
    instance_id: str
    trace_id: Optional[str]
    span_id: Optional[str]
    message: str
    fields: Dict[str, Any]
    
    def to_json(self) -> str:
        return json.dumps(asdict(self), ensure_ascii=False, separators=(',', ':'))

class SensitiveDataMasker:
    """æ•æ„Ÿæ•°æ®è„±æ•å™¨"""
    
    def __init__(self):
        self.patterns = {
            'phone': re.compile(r'1[3-9]\d{9}'),
            'email': re.compile(r'[a-zA-Z0-9._%+-]+@[a-zA-Z0-9.-]+\.[a-zA-Z]{2,}'),
            'id_card': re.compile(r'\d{17}[\dXx]'),
            'credit_card': re.compile(r'\d{4}[- ]?\d{4}[- ]?\d{4}[- ]?\d{4}'),
            'password': re.compile(r'password["\']?\s*[:=]\s*["\']?([^"\']+)["\']?', re.IGNORECASE),
        }
        
        self.sensitive_fields = {
            'password', 'passwd', 'secret', 'token', 'key',
            'phone', 'mobile', 'telephone', 'email', 'id_card'
        }
    
    def mask_data(self, data: Any) -> Any:
        """é€’å½’è„±æ•æ•°æ®"""
        if isinstance(data, dict):
            return {k: self.mask_data(v) if not self._is_sensitive_field(k) 
                   else self._mask_value(v) for k, v in data.items()}
        elif isinstance(data, list):
            return [self.mask_data(item) for item in data]
        elif isinstance(data, str):
            return self._mask_string(data)
        else:
            return data
    
    def _is_sensitive_field(self, field_name: str) -> bool:
        """åˆ¤æ–­æ˜¯å¦ä¸ºæ•æ„Ÿå­—æ®µ"""
        return field_name.lower() in self.sensitive_fields
    
    def _mask_value(self, value: Any) -> str:
        """è„±æ•å­—æ®µå€¼"""
        if isinstance(value, str):
            if len(value) <= 4:
                return '*' * len(value)
            else:
                return value[:2] + '*' * (len(value) - 4) + value[-2:]
        else:
            return str(value)
    
    def _mask_string(self, text: str) -> str:
        """è„±æ•å­—ç¬¦ä¸²ä¸­çš„æ•æ„Ÿä¿¡æ¯"""
        for pattern_name, pattern in self.patterns.items():
            text = pattern.sub(lambda m: self._mask_match(m.group()), text)
        return text
    
    def _mask_match(self, match: str) -> str:
        """è„±æ•åŒ¹é…çš„å†…å®¹"""
        if len(match) <= 4:
            return '*' * len(match)
        else:
            return match[:2] + '*' * (len(match) - 4) + match[-2:]

class RingBuffer:
    """é«˜æ€§èƒ½ç¯å½¢ç¼“å†²åŒº"""
    
    def __init__(self, size: int):
        self.size = size
        self.buffer = [None] * size
        self.head = 0
        self.tail = 0
        self.count = 0
        self.lock = threading.Lock()
    
    def put(self, item: Any) -> bool:
        """æ·»åŠ å…ƒç´ ï¼Œè¿”å›æ˜¯å¦æˆåŠŸ"""
        with self.lock:
            if self.count >= self.size:
                return False  # ç¼“å†²åŒºå·²æ»¡
            
            self.buffer[self.tail] = item
            self.tail = (self.tail + 1) % self.size
            self.count += 1
            return True
    
    def get_batch(self, max_size: int) -> List[Any]:
        """æ‰¹é‡è·å–å…ƒç´ """
        with self.lock:
            if self.count == 0:
                return []
            
            batch_size = min(max_size, self.count)
            batch = []
            
            for _ in range(batch_size):
                batch.append(self.buffer[self.head])
                self.head = (self.head + 1) % self.size
                self.count -= 1
            
            return batch

class LogWriter:
    """æ—¥å¿—å†™å…¥å™¨"""
    
    def __init__(self, file_path: str, max_file_size: int = 100 * 1024 * 1024):
        self.file_path = Path(file_path)
        self.max_file_size = max_file_size
        self.current_file = None
        self.current_size = 0
        self.rotation_count = 0
        
        # ç¡®ä¿ç›®å½•å­˜åœ¨
        self.file_path.parent.mkdir(parents=True, exist_ok=True)
        self._open_file()
    
    def _open_file(self):
        """æ‰“å¼€æ—¥å¿—æ–‡ä»¶"""
        if self.current_file:
            self.current_file.close()
        
        if self.file_path.exists():
            self.current_size = self.file_path.stat().st_size
        else:
            self.current_size = 0
        
        self.current_file = open(self.file_path, 'a', encoding='utf-8', buffering=8192)
    
    def write_batch(self, records: List[LogRecord]):
        """æ‰¹é‡å†™å…¥æ—¥å¿—è®°å½•"""
        if not records:
            return
        
        # æ£€æŸ¥æ˜¯å¦éœ€è¦è½®è½¬
        if self._should_rotate():
            self._rotate_file()
        
        # æ‰¹é‡å†™å…¥
        batch_data = '\n'.join(record.to_json() for record in records) + '\n'
        self.current_file.write(batch_data)
        self.current_file.flush()
        
        self.current_size += len(batch_data.encode('utf-8'))
    
    def _should_rotate(self) -> bool:
        """åˆ¤æ–­æ˜¯å¦éœ€è¦è½®è½¬æ–‡ä»¶"""
        return self.current_size >= self.max_file_size
    
    def _rotate_file(self):
        """è½®è½¬æ—¥å¿—æ–‡ä»¶"""
        if self.current_file:
            self.current_file.close()
        
        # é‡å‘½åå½“å‰æ–‡ä»¶
        timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
        rotated_name = f"{self.file_path.stem}_{timestamp}_{self.rotation_count}.log"
        rotated_path = self.file_path.parent / rotated_name
        
        if self.file_path.exists():
            self.file_path.rename(rotated_path)
        
        self.rotation_count += 1
        self.current_size = 0
        self._open_file()
    
    def close(self):
        """å…³é—­æ–‡ä»¶"""
        if self.current_file:
            self.current_file.close()

class AsyncLogger:
    """å¼‚æ­¥æ—¥å¿—å™¨"""
    
    def __init__(self, service_name: str, instance_id: str = None,
                 buffer_size: int = 10000, batch_size: int = 100,
                 flush_interval: float = 1.0, log_file: str = "app.log"):
        
        self.service_name = service_name
        self.instance_id = instance_id or str(uuid.uuid4())[:8]
        self.batch_size = batch_size
        self.flush_interval = flush_interval
        
        # åˆå§‹åŒ–ç»„ä»¶
        self.buffer = RingBuffer(buffer_size)
        self.writer = LogWriter(log_file)
        self.masker = SensitiveDataMasker()
        
        # çº¿ç¨‹æ§åˆ¶
        self.running = True
        self.writer_thread = threading.Thread(target=self._writer_loop, daemon=True)
        self.writer_thread.start()
        
        # ç»Ÿè®¡ä¿¡æ¯
        self.stats = {
            'total_logs': 0,
            'dropped_logs': 0,
            'batches_written': 0,
            'last_flush_time': time.time()
        }
    
    def log(self, level: LogLevel, message: str, **fields):
        """è®°å½•æ—¥å¿—"""
        # è·å–è¿½è¸ªä¿¡æ¯
        trace_id = fields.pop('trace_id', None)
        span_id = fields.pop('span_id', None)
        
        # è„±æ•æ•æ„Ÿæ•°æ®
        masked_fields = self.masker.mask_data(fields)
        masked_message = self.masker._mask_string(message)
        
        # åˆ›å»ºæ—¥å¿—è®°å½•
        record = LogRecord(
            timestamp=datetime.utcnow().isoformat() + 'Z',
            level=level.name,
            service=self.service_name,
            instance_id=self.instance_id,
            trace_id=trace_id,
            span_id=span_id,
            message=masked_message,
            fields=masked_fields
        )
        
        # æ·»åŠ åˆ°ç¼“å†²åŒº
        if not self.buffer.put(record):
            self.stats['dropped_logs'] += 1
        else:
            self.stats['total_logs'] += 1
    
    def _writer_loop(self):
        """å†™å…¥å¾ªç¯"""
        while self.running:
            try:
                # è·å–æ‰¹é‡è®°å½•
                records = self.buffer.get_batch(self.batch_size)
                
                if records:
                    # æ‰¹é‡å†™å…¥
                    self.writer.write_batch(records)
                    self.stats['batches_written'] += 1
                    self.stats['last_flush_time'] = time.time()
                else:
                    # æ²¡æœ‰è®°å½•æ—¶çŸ­æš‚ä¼‘çœ 
                    time.sleep(0.01)
                
                # å®šæœŸå¼ºåˆ¶åˆ·æ–°
                if time.time() - self.stats['last_flush_time'] > self.flush_interval:
                    remaining_records = self.buffer.get_batch(self.buffer.count)
                    if remaining_records:
                        self.writer.write_batch(remaining_records)
                        self.stats['batches_written'] += 1
                    self.stats['last_flush_time'] = time.time()
                    
            except Exception as e:
                print(f"æ—¥å¿—å†™å…¥é”™è¯¯: {e}")
                time.sleep(0.1)
    
    def info(self, message: str, **fields):
        self.log(LogLevel.INFO, message, **fields)
    
    def debug(self, message: str, **fields):
        self.log(LogLevel.DEBUG, message, **fields)
    
    def warning(self, message: str, **fields):
        self.log(LogLevel.WARN, message, **fields)
    
    def error(self, message: str, **fields):
        self.log(LogLevel.ERROR, message, **fields)
    
    def fatal(self, message: str, **fields):
        self.log(LogLevel.FATAL, message, **fields)
    
    def get_stats(self) -> Dict[str, Any]:
        """è·å–ç»Ÿè®¡ä¿¡æ¯"""
        return {
            **self.stats,
            'buffer_usage': self.buffer.count / self.buffer.size,
            'uptime': time.time() - self.stats.get('start_time', time.time())
        }
    
    def close(self):
        """å…³é—­æ—¥å¿—å™¨"""
        self.running = False
        self.writer_thread.join(timeout=5.0)
        
        # åˆ·æ–°å‰©ä½™æ—¥å¿—
        remaining_records = self.buffer.get_batch(self.buffer.count)
        if remaining_records:
            self.writer.write_batch(remaining_records)
        
        self.writer.close()

# æ—¥å¿—ä¸Šä¸‹æ–‡ç®¡ç†å™¨
class LogContext:
    """æ—¥å¿—ä¸Šä¸‹æ–‡ç®¡ç†å™¨"""
    
    def __init__(self):
        self._local = threading.local()
    
    def set_trace_id(self, trace_id: str):
        """è®¾ç½®è¿½è¸ªID"""
        self._local.trace_id = trace_id
    
    def set_span_id(self, span_id: str):
        """è®¾ç½®Span ID"""
        self._local.span_id = span_id
    
    def get_trace_id(self) -> Optional[str]:
        """è·å–è¿½è¸ªID"""
        return getattr(self._local, 'trace_id', None)
    
    def get_span_id(self) -> Optional[str]:
        """è·å–Span ID"""
        return getattr(self._local, 'span_id', None)
    
    def clear(self):
        """æ¸…é™¤ä¸Šä¸‹æ–‡"""
        self._local.trace_id = None
        self._local.span_id = None

# å…¨å±€æ—¥å¿—ä¸Šä¸‹æ–‡
log_context = LogContext()

class ContextualLogger:
    """å¸¦ä¸Šä¸‹æ–‡çš„æ—¥å¿—å™¨"""
    
    def __init__(self, async_logger: AsyncLogger):
        self.async_logger = async_logger
    
    def _add_context(self, fields: Dict[str, Any]) -> Dict[str, Any]:
        """æ·»åŠ ä¸Šä¸‹æ–‡ä¿¡æ¯"""
        context_fields = {}
        
        if log_context.get_trace_id():
            context_fields['trace_id'] = log_context.get_trace_id()
        
        if log_context.get_span_id():
            context_fields['span_id'] = log_context.get_span_id()
        
        return {**context_fields, **fields}
    
    def info(self, message: str, **fields):
        self.async_logger.info(message, **self._add_context(fields))
    
    def debug(self, message: str, **fields):
        self.async_logger.debug(message, **self._add_context(fields))
    
    def warning(self, message: str, **fields):
        self.async_logger.warning(message, **self._add_context(fields))
    
    def error(self, message: str, **fields):
        self.async_logger.error(message, **self._add_context(fields))
    
    def fatal(self, message: str, **fields):
        self.async_logger.fatal(message, **self._add_context(fields))

# ä½¿ç”¨ç¤ºä¾‹å’Œæ€§èƒ½æµ‹è¯•
def performance_test():
    """æ€§èƒ½æµ‹è¯•"""
    import time
    import threading
    from concurrent.futures import ThreadPoolExecutor
    
    # åˆ›å»ºæ—¥å¿—å™¨
    async_logger = AsyncLogger(
        service_name="test-service",
        buffer_size=50000,
        batch_size=500,
        flush_interval=0.5
    )
    
    contextual_logger = ContextualLogger(async_logger)
    
    def log_worker(worker_id: int, log_count: int):
        """æ—¥å¿—å·¥ä½œçº¿ç¨‹"""
        log_context.set_trace_id(f"trace-{worker_id}")
        
        for i in range(log_count):
            log_context.set_span_id(f"span-{worker_id}-{i}")
            
            contextual_logger.info(
                f"å¤„ç†è¯·æ±‚ {i}",
                worker_id=worker_id,
                request_id=f"req-{i}",
                user_id=12345,
                email="user@example.com",
                phone="13800138000",
                password="secret123"  # å°†è¢«è„±æ•
            )
            
            if i % 100 == 0:
                contextual_logger.warning(
                    f"å¤„ç†è¿›åº¦æ£€æŸ¥",
                    worker_id=worker_id,
                    progress=i / log_count * 100
                )
    
    # æ€§èƒ½æµ‹è¯•
    start_time = time.time()
    
    with ThreadPoolExecutor(max_workers=10) as executor:
        futures = []
        for worker_id in range(10):
            future = executor.submit(log_worker, worker_id, 1000)
            futures.append(future)
        
        # ç­‰å¾…æ‰€æœ‰ä»»åŠ¡å®Œæˆ
        for future in futures:
            future.result()
    
    # ç­‰å¾…æ—¥å¿—å†™å…¥å®Œæˆ
    time.sleep(2)
    
    end_time = time.time()
    duration = end_time - start_time
    
    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    stats = async_logger.get_stats()
    print(f"æ€§èƒ½æµ‹è¯•ç»“æœ:")
    print(f"  æ€»è€—æ—¶: {duration:.2f}ç§’")
    print(f"  æ€»æ—¥å¿—æ•°: {stats['total_logs']}")
    print(f"  ä¸¢å¤±æ—¥å¿—æ•°: {stats['dropped_logs']}")
    print(f"  å†™å…¥æ‰¹æ¬¡: {stats['batches_written']}")
    print(f"  ååé‡: {stats['total_logs'] / duration:.0f} logs/sec")
    print(f"  ç¼“å†²åŒºä½¿ç”¨ç‡: {stats['buffer_usage']:.2%}")
    
    # å…³é—­æ—¥å¿—å™¨
    async_logger.close()

if __name__ == "__main__":
    performance_test()
```

## ğŸ¯ é¢è¯•è¦ç‚¹æ€»ç»“

### æŠ€æœ¯æ·±åº¦ä½“ç°

**æ—¥å¿—ç³»ç»Ÿæ ¸å¿ƒæŠ€æœ¯**ï¼š
- å¼‚æ­¥å†™å…¥å’Œæ‰¹é‡å¤„ç†çš„å®ç°åŸç†
- ç¯å½¢ç¼“å†²åŒºçš„æ— é”è®¾è®¡
- å†…å­˜æ˜ å°„æ–‡ä»¶I/Oä¼˜åŒ–
- æ—¥å¿—è½®è½¬å’Œå­˜å‚¨ç®¡ç†ç­–ç•¥

**æ€§èƒ½ä¼˜åŒ–ç­–ç•¥**ï¼š
- å¤šçº§ç¼“å†²å‡å°‘é”ç«äº‰
- æ‰¹é‡å†™å…¥é™ä½I/Oå¼€é”€
- èƒŒå‹æ§åˆ¶é˜²æ­¢å†…å­˜æº¢å‡º
- æ™ºèƒ½åˆ·æ–°ç­–ç•¥å¹³è¡¡æ€§èƒ½å’Œå¯é æ€§

### ç”Ÿäº§å®è·µç»éªŒ

**å¯é æ€§ä¿éšœ**ï¼š
- æ—¥å¿—ä¸¢å¤±çš„é¢„é˜²å’Œç›‘æ§
- ç£ç›˜ç©ºé—´ç®¡ç†å’Œæ¸…ç†ç­–ç•¥
- æ—¥å¿—å®Œæ•´æ€§æ ¡éªŒæœºåˆ¶
- æ•…éšœæ¢å¤å’Œé™çº§æ–¹æ¡ˆ

**å®‰å…¨åˆè§„è€ƒè™‘**ï¼š
- æ•æ„Ÿä¿¡æ¯è‡ªåŠ¨è„±æ•
- æ—¥å¿—è®¿é—®æƒé™æ§åˆ¶
- æ•°æ®ä¿ç•™å’Œé”€æ¯ç­–ç•¥
- å®¡è®¡æ—¥å¿—çš„ç‰¹æ®Šå¤„ç†

### é¢è¯•å›ç­”è¦ç‚¹

**æ¶æ„è®¾è®¡èƒ½åŠ›**ï¼š
- åŸºäºä¸šåŠ¡éœ€æ±‚è®¾è®¡æ—¥å¿—æ¶æ„
- è€ƒè™‘æ‰©å±•æ€§å’Œç»´æŠ¤æ€§
- ä¸ç›‘æ§ç³»ç»Ÿçš„é›†æˆè®¾è®¡
- æˆæœ¬æ•ˆç›Šçš„æƒè¡¡åˆ†æ

**é—®é¢˜è§£å†³æ€è·¯**ï¼š
- æ€§èƒ½ç“¶é¢ˆçš„è¯†åˆ«å’Œä¼˜åŒ–
- æ—¥å¿—æ•°æ®çš„æŸ¥è¯¢å’Œåˆ†æ
- å¼‚å¸¸æƒ…å†µçš„å¤„ç†ç­–ç•¥
- ç³»ç»Ÿç›‘æ§å’Œå‘Šè­¦è®¾è®¡

---

[â† è¿”å›ç›‘æ§ä¸è°ƒè¯•é¢è¯•é¢˜](../../questions/backend/monitoring-debugging.md) 